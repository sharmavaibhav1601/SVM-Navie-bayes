{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions"
      ],
      "metadata": {
        "id": "bMvEO0fOexRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: What is a Support Vector Machine (SVM)?**  \n",
        "SVM is a supervised learning algorithm used for classification and regression. It finds the optimal hyperplane that maximizes the margin between different classes in a dataset. It works well in high-dimensional spaces and can handle non-linearly separable data using kernel functions.\n",
        "\n",
        "---\n",
        "\n",
        "**2: What is the difference between Hard Margin and Soft Margin SVM?**  \n",
        "Hard Margin SVM strictly separates data with no misclassification, making it sensitive to noise. Soft Margin SVM allows some misclassification by introducing a slack variable, balancing margin maximization and classification accuracy, making it more suitable for real-world, noisy datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**3: What is the mathematical intuition behind SVM?**  \n",
        "SVM aims to find a hyperplane \\( w \\cdot x + b = 0 \\) that maximizes the margin between two classes. It minimizes \\( ||w||^2 \\) subject to correct classification constraints. When data is non-linearly separable, kernel functions map it to a higher-dimensional space for separation.\n",
        "\n",
        "---\n",
        "\n",
        "**4: What is the role of Lagrange Multipliers in SVM?**  \n",
        "Lagrange multipliers help transform the constrained optimization problem into an unconstrained one using the Lagrangian function. They allow SVM to maximize the margin while enforcing classification constraints through the Karush-Kuhn-Tucker (KKT) conditions.\n",
        "\n",
        "---\n",
        "\n",
        "**5: What are Support Vectors in SVM?**  \n",
        "Support vectors are data points that lie closest to the decision boundary (hyperplane). They determine the position and orientation of the hyperplane and are crucial for maximizing the margin between classes.\n",
        "\n",
        "---\n",
        "\n",
        "**6: What is a Support Vector Classifier (SVC)?**  \n",
        "SVC is the classification variant of SVM that finds an optimal hyperplane for separating different classes. It can handle both linearly and non-linearly separable data using kernel functions.\n",
        "\n",
        "---\n",
        "\n",
        "**7: What is a Support Vector Regressor (SVR)?**  \n",
        "SVR applies SVM to regression tasks by fitting a hyperplane within a margin of tolerance (ε) around the actual data points. It minimizes prediction error while avoiding unnecessary complexity.\n",
        "\n",
        "---\n",
        "\n",
        "**8: What is the Kernel Trick in SVM?**  \n",
        "The kernel trick maps input data to a higher-dimensional space where it becomes linearly separable, without explicitly computing the transformation. Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "---\n",
        "\n",
        "**9: Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.**  \n",
        "- **Linear Kernel:** Best for linearly separable data, simple and fast.  \n",
        "- **Polynomial Kernel:** Captures complex relationships, but computationally expensive.  \n",
        "- **RBF Kernel:** Handles highly non-linear data by mapping it to infinite dimensions, but requires tuning.\n",
        "\n",
        "---\n",
        "\n",
        "**10: What is the effect of the C parameter in SVM?**  \n",
        "The **C parameter** controls the trade-off between margin size and classification accuracy. A **high C** focuses on minimizing misclassification, risking overfitting. A **low C** allows a larger margin but may misclassify more points, improving generalization.\n",
        "\n",
        "---\n",
        "\n",
        "**11: What is the role of the Gamma parameter in RBF Kernel SVM?**  \n",
        "Gamma controls how far the influence of a single training point reaches. **High gamma** captures fine details but risks overfitting, while **low gamma** generalizes better but may underfit.\n",
        "\n",
        "---\n",
        "\n",
        "**12: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**  \n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' Theorem, assuming feature independence. It’s called \"naïve\" because it assumes that all features contribute independently to the outcome, which may not hold in real-world data.\n",
        "\n",
        "---\n",
        "\n",
        "**13: What is Bayes’ Theorem?**  \n",
        "Bayes’ Theorem states that:  \n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "\\]\n",
        "It calculates the probability of event A given event B, based on prior probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "**14: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**  \n",
        "- **Gaussian NB:** Assumes features follow a normal distribution. Best for continuous data.  \n",
        "- **Multinomial NB:** Works with frequency-based discrete data. Used in text classification.  \n",
        "- **Bernoulli NB:** Handles binary feature data, often applied in spam detection.\n",
        "\n",
        "---\n",
        "\n",
        "**15: When should you use Gaussian Naïve Bayes over other variants?**  \n",
        "Use Gaussian NB when features are continuous and follow a normal distribution, such as in medical diagnosis or sensor data classification.\n",
        "\n",
        "---\n",
        "\n",
        "**16: What are the key assumptions made by Naïve Bayes?**  \n",
        "1. **Feature independence** (unlikely in real-world data).  \n",
        "2. **Equal importance of all features.**  \n",
        "3. **Correctness of prior probabilities.**\n",
        "\n",
        "---\n",
        "\n",
        "**17: What are the advantages and disadvantages of Naïve Bayes?**  \n",
        "**Advantages:** Fast, requires little training data, works well with high-dimensional data, and is easy to interpret.  \n",
        "**Disadvantages:** Assumes feature independence, struggles with correlated features, and may not perform well on complex datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**18: Why is Naïve Bayes a good choice for text classification?**  \n",
        "Naïve Bayes is effective for text classification because it efficiently handles high-dimensional sparse data, works well with bag-of-words models, and provides fast predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**19: Compare SVM and Naïve Bayes for classification tasks.**  \n",
        "- **SVM:** Works well with complex, high-dimensional data but is computationally expensive.  \n",
        "- **Naïve Bayes:** Fast, simple, and effective for text data but relies on strong independence assumptions.\n",
        "\n",
        "---\n",
        "\n",
        "**20: How does Laplace Smoothing help in Naïve Bayes?**  \n",
        "Laplace Smoothing prevents zero probabilities by adding a small constant to all feature counts. It ensures unseen words or features do not completely invalidate the probability estimation."
      ],
      "metadata": {
        "id": "_0risye7e2kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "EtYZFo2oky5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21: Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM Classifier on Iris dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "KmvTpDxyk4Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM Classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy of SVM Classifier with Linear kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy of SVM Classifier with RBF kernel: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "7pGJ9QeQlwhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23: Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM Regressor\n",
        "svr = SVR(kernel='linear')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error of SVR on housing dataset: {mse:.2f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-LHpH9qPmpQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24: Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a synthetic dataset for visualization\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Create a mesh to plot the decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),\n",
        "                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plotting\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "plt.title('SVM Classifier with Polynomial Kernel')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdfWH-Qqnwuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "qSnTLWcAoNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26: Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "data = fetch_20newsgroups(subset='train')\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_data = fetch_20newsgroups(subset='test')\n",
        "y_pred = model.predict(test_data.data)\n",
        "accuracy = accuracy_score(test_data.target, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Multinomial Naïve Bayes on 20 Newsgroups dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "pgPfxjcJoiFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n",
        "# Create a synthetic dataset for visualization\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Train SVM Classifier with different C values\n",
        "for i, C in enumerate([0.1, 1, 10]):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    # Create a mesh to plot the decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),\n",
        "                         np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plotting\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "    plt.title(f'SVM Classifier with C={C}')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXSrzPW8o39L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features.\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a synthetic binary dataset\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_classes=2, n_informative=2, random_state=42)\n",
        "\n",
        "# Train the Bernoulli Naïve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bnb.predict(X)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(f\"Accuracy of Bernoulli Naïve Bayes: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "2aBDEZfzpyFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with scaling\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "l0Ysg5O1p6hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing.\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes without smoothing\n",
        "gnb_no_smoothing = GaussianNB()\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Train Gaussian Naïve Bayes with Laplace Smoothing\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.2f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.2f}\")"
      ],
      "metadata": {
        "id": "wTC4Y9zZqR4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Define the model\n",
        "svm = SVC()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best parameters and score\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "kayx3fTYrnYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32 Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with class weighting\n",
        "svm_weighted = SVC(class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(f\"Accuracy of SVM with class weighting: {accuracy_weighted:.2f}\")"
      ],
      "metadata": {
        "id": "S8VFp1Gor21k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33 Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample email data\n",
        "emails = [\n",
        "    \"Free money now!!!\",\n",
        "    \"Hi, how are you?\",\n",
        "    \"Win a free ticket to Bahamas!\",\n",
        "    \"Meeting at 10am tomorrow.\",\n",
        "    \"Congratulations! You've won a lottery!\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1]  # 1 for spam, 0 for not spam\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(emails, labels)\n",
        "\n",
        "# Test the model\n",
        "test_emails = [\"Get rich quick!\", \"Let's have lunch tomorrow.\"]\n",
        "predictions = model.predict(test_emails)\n",
        "\n",
        "print(\"Predictions:\", predictions)  # Output: [1, 0] (spam, not spam)"
      ],
      "metadata": {
        "id": "JHUqFNb9sFkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34 Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "\n",
        "# Train a Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
        "\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n",
        "print(f\"Naïve Bayes Accuracy: {nb_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "VjJOlPK8sRVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35 Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Perform feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train Gaussian Naïve Bayes on selected features\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_selected, y_train)\n",
        "y_pred_selected = gnb.predict(X_test_selected)\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "# Train Gaussian Naïve Bayes on all features\n",
        "gnb_full = GaussianNB()\n",
        "gnb_full.fit(X_train, y_train)\n",
        "y_pred_full = gnb_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy with feature selection: {accuracy_selected:.2f}\")\n",
        "print(f\"Accuracy without feature selection: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "o29RUnT1scyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36 Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with One-vs-Rest\n",
        "ovr_classifier = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# Train SVM Classifier with One-vs-One\n",
        "ovo_classifier = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo_classifier.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy of SVM with One-vs-Rest: {accuracy_ovr:.2f}\")\n",
        "print(f\"Accuracy of SVM with One-vs-One: {accuracy_ovo:.2f}\")"
      ],
      "metadata": {
        "id": "w2BjaxKatpqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM Classifier with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# Train SVM Classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy of SVM with Linear kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy of SVM with Polynomial kernel: {accuracy_poly:.2f}\")\n",
        "print(f\"Accuracy of SVM with RBF kernel: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "eW6-uPgOuDg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Set up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    svm = SVC(kernel='linear')\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Compute average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f\"Average accuracy using Stratified K-Fold: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "OzqRLLiCut4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance.\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes with default priors\n",
        "gnb_default = GaussianNB()\n",
        "gnb_default.fit(X_train, y_train)\n",
        "y_pred_default = gnb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train Gaussian Naïve Bayes with custom priors\n",
        "priors = [0.6, 0.4]  # Example of custom priors\n",
        "gnb_custom = GaussianNB(priors=priors)\n",
        "gnb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = gnb_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy with default priors: {accuracy_default:.2f}\")\n",
        "print(f\"Accuracy with custom priors: {accuracy_custom:.2f}\")"
      ],
      "metadata": {
        "id": "XtuoL3ehu6IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Perform Recursive Feature Elimination\n",
        "svm = SVC(kernel='linear')\n",
        "selector = RFE(svm, n_features_to_select=10)\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "# Train SVM on selected features\n",
        "svm.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Train SVM on all features\n",
        "svm_full = SVC(kernel='linear')\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy with RFE: {accuracy_rfe:.2f}\")\n",
        "print(f\"Accuracy without RFE: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "id": "H4B_eZJzvMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "byrzQjhqvYwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = gnb.predict_proba(X_test)\n",
        "\n",
        "# Evaluate using Log Loss\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "print(f\"Log Loss of Naïve Bayes Classifier: {loss:.2f}\")"
      ],
      "metadata": {
        "id": "RAszlHfjvl4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43 Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "maMlboB3vwR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44.Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the SVR model with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "svr_models = {}\n",
        "mae_scores = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    # Create and train the SVR model\n",
        "    svr = SVR(kernel=kernel)\n",
        "    svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate MAE\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Store the model and score\n",
        "    svr_models[kernel] = svr\n",
        "    mae_scores[kernel] = mae\n",
        "\n",
        "    print(f\"SVR with {kernel} kernel - MAE: {mae:.4f}\")\n",
        "\n",
        "# Find the best kernel based on MAE\n",
        "best_kernel = min(mae_scores, key=mae_scores.get)\n",
        "print(f\"\\nBest kernel: {best_kernel} with MAE: {mae_scores[best_kernel]:.4f}\")\n",
        "\n",
        "# Fine-tune the best model using GridSearchCV\n",
        "if best_kernel == 'rbf':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': [0.01, 0.1, 1, 'scale', 'auto'],\n",
        "        'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
        "    }\n",
        "elif best_kernel == 'poly':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'degree': [2, 3, 4],\n",
        "        'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
        "    }\n",
        "else:  # linear\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
        "    }\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    SVR(kernel=best_kernel),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_score = -grid_search.best_score_  # Convert back to positive MAE\n",
        "\n",
        "print(f\"\\nBest parameters: {best_params}\")\n",
        "print(f\"Best cross-validation MAE: {best_score:.4f}\")\n",
        "\n",
        "# Evaluate the optimized model on the test set\n",
        "best_svr = grid_search.best_estimator_\n",
        "y_pred_best = best_svr.predict(X_test_scaled)\n",
        "final_mae = mean_absolute_error(y_test, y_pred_best)\n",
        "print(f\"Final test set MAE with optimized model: {final_mae:.4f}\")\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Sort the data for better visualization\n",
        "sort_idx = np.argsort(X_test.flatten())\n",
        "X_test_sorted = X_test[sort_idx]\n",
        "y_test_sorted = y_test[sort_idx]\n",
        "y_pred_best_sorted = y_pred_best[sort_idx]\n",
        "\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual data')\n",
        "plt.plot(X_test_sorted, y_pred_best_sorted, color='red', linewidth=2, label=f'SVR prediction (MAE: {final_mae:.4f})')\n",
        "plt.title(f'SVR Regression with {best_kernel} kernel (Optimized)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jLXurD6wv5xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = gnb.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Evaluate using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC score of Naïve Bayes Classifier: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "XWAMliQewPqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear', probability=True)  # Enable probability estimates\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_scores = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dg16RjfnwbyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}